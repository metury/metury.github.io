<h4 id="info"><em>INFO</em></h4>
<p><em>This is shortened text from Probability and statistics 2. Almost
all examples are omitted since this is just shortened version. Also you
may find several notes of what was omitted and what is the reasoning
behind that. Usually it is the lack of my time.</em></p>
<h1 id="markov-chains">Markov chains</h1>
<ul>
<li><em>â€œSome type of an automat, that represent probability space. It
needs to have special properties.â€</em></li>
</ul>
<pre class="mermaid"><code>graph LR;
    id1((Machine working)) -- &quot;0.9&quot; --&gt; id1;
    id1 -- &quot;0.1&quot; --&gt; id2((Machine not working));
    id2 -- &quot;0.99&quot; --&gt; id1;
    id2 -- &quot;0.01&quot; --&gt; id2;</code></pre>
<h2 id="model">Model</h2>
<ul>
<li>States: <span class="math inline"><em>S</em></span> it is usually
finite and sometimes only countable.</li>
<li>sequence <span
class="math inline"><em>X</em><sub>0</sub>,â€†<em>X</em><sub>1</sub>,â€†<em>X</em><sub>2</sub>,â€†â€¦</span>
of random variables with values in <span
class="math inline"><em>S</em></span></li>
<li><span class="math inline"><em>X</em><sub><em>t</em>â€…+â€…1</sub></span>
depends <strong>only</strong> on <span
class="math inline"><em>X</em><sub><em>t</em></sub></span></li>
<li><span
class="math inline">Prâ€†[<em>X</em><sub><em>t</em>â€…+â€…1</sub>=<em>j</em>|<em>X</em><sub><em>t</em></sub>=<em>i</em>]â€„=â€„<em>p</em><sub><em>i</em><em>j</em></sub></span>
where <span
class="math inline"><em>i</em>,â€†<em>j</em>â€„âˆˆâ€„<em>S</em></span></li>
</ul>
<h2 id="definition">Definition:</h2>
<p>Sequence of r.v. <span
class="math inline"><em>X</em><sub>0</sub>,â€†<em>X</em><sub>1</sub>,â€†<em>X</em><sub>2</sub>,â€†â€¦</span>
is a <strong>Markov chain</strong> if:</p>
<ul>
<li><span class="math inline">$\exist$</span> countable <span
class="math inline"><em>S</em>â€„:â€„Rng
<em>X</em><sub><em>t</em></sub>â€„âŠ‚â€„<em>S</em></span> Â <span
class="math inline">âˆ€<em>t</em></span></li>
<li><span class="math inline">âˆ€<em>t</em>â€„âˆˆâ€„â„•</span> <span
class="math inline">âˆ€<em>a</em><sub>0</sub>,â€†<em>a</em><sub>1</sub>,â€†<em>a</em><sub>2</sub>,â€†â€¦,â€†<em>a</em><sub><em>t</em>â€…+â€…1</sub>â€„âˆˆâ€„<em>S</em></span></li>
</ul>
<p>$$ = </p>
<p>$$</p>
<p>This means that Markov chain has the property of being memory-less
and this probability written above is called <em>transition
probability</em>. We can map all elements from <span
class="math inline"><em>S</em></span> to a number from range <span
class="math inline">1,â€†2,â€†â€¦,â€†<em>n</em></span> and then we can build
<strong>transition matrix</strong>.</p>
$$ P =
<p>$$</p>
<p>Where <span
class="math inline"><em>p</em><sub><em>i</em><em>j</em></sub></span>
means going from <span class="math inline"><em>i</em></span> to <span
class="math inline"><em>j</em></span>. All <span
class="math inline"><em>p</em><sub><em>i</em><em>j</em></sub>â€„â‰¥â€„0</span>
and the sum of each row is <span class="math inline">1</span>. Also we
can build <strong>transition graph</strong> representing this Markov
chain. In that graph <span
class="math inline"><em>V</em>â€„=â€„<em>S</em></span> and arcs exists if
<span
class="math inline">(<em>i</em><em>j</em>)â€„:â€„<em>p</em><sub><em>i</em><em>j</em></sub>â€„&gt;â€„0</span>.</p>
<p>Now we look at distribution, or PMF of <span
class="math inline"><em>X</em><sub><em>k</em></sub>â€„=â€„<em>Ï€</em><sup>(<em>k</em>)</sup></span>
where <span
class="math inline"><em>Ï€</em><sup>(<em>k</em>)</sup>â€„=â€„(<em>Ï€</em><sub>1</sub><sup>(<em>k</em>)</sup>,<em>Ï€</em><sub>2</sub><sup>(<em>k</em>)</sup>,<em>Ï€</em><sub>3</sub><sup>(<em>k</em>)</sup>,â€¦)</span>
and the sum is <span class="math inline">1</span>. Then we may see that
<span
class="math inline"><em>Ï€</em><sub><em>j</em></sub><sup>(<em>k</em>)</sup>â€„=â€„Prâ€†[<em>X</em><sub><em>k</em></sub>=<em>j</em>]</span>.
We will be calling <span
class="math inline"><em>Ï€</em><sup>(0)</sup></span> an <em>initial
state</em>.</p>
<p>Then we can see that <span
class="math inline"><em>Ï€</em><sup>(1)</sup>â€„=â€„<em>Ï€</em><sup>(0)</sup><em>P</em></span>
as multiplication by transition matrix. We can generalize this to:</p>
<p>$$ ^{(k)} = ^{(k-1)} P</p>
<p>$$</p>
<h2 id="theorem">Theorem:</h2>
<p>For any <em>MC</em> with transition matrix <span
class="math inline"><em>P</em></span> we have <span
class="math inline"><em>Ï€</em><sup>(<em>k</em>)</sup>â€„=â€„<em>Ï€</em><sup>(0)</sup><em>P</em><sup><em>k</em></sup></span>
and <span
class="math inline"><em>Ï€</em><sup>(<em>k</em>+1)</sup>â€„=â€„<em>Ï€</em><sup>(<em>t</em>)</sup><em>P</em><sup><em>k</em></sup></span>.</p>
<h3 id="proof">Proof</h3>
<p><em>Proof will be by induction. So <span
class="math inline"><em>Ï€</em><sup>(<em>k</em>+1)</sup>â€„=â€„<em>Ï€</em><sup>(<em>k</em>)</sup><em>P</em>â€„=â€„<em>Ï€</em><sup>(0)</sup><em>P</em><sup><em>k</em></sup><em>P</em>â€„=â€„<em>Ï€</em><sup>(0)</sup><em>P</em><sup><em>k</em>â€…+â€…1</sup></span>.</em></p>
<p>$$ </p>
<p>$$</p>
<h2 id="definition-1">Definition:</h2>
<p><strong>K-step transition</strong> is defined as:</p>
$$
<p>$$</p>
<h2 id="observation">Observation</h2>
<p>$$ r_{ij}(k) = _{j}^{(k)} ^{0} = ( 0, 0, , 0, 1, 0 , 0)</p>
<p>$$</p>
<p>Where <span class="math inline">1</span> is on <span
class="math inline"><em>i</em></span>-th position. Also:</p>
<p>$$ <em>{j}^{(k)} = (^{0} P^{k})</em>{j} = ( ( 0, 0, , 0, 1, 0 ,
0)P^{k})<em>{j} = (P^{k})</em>{ij}</p>
<p>$$</p>
<h2 id="chapman-kologorov-formula">Chapman-Kologorov formula</h2>
<p>$$ r_{ij}(k) = (P^{k})<em>{ij} \ r</em>{ij}(k+l) = <em>{t=1}^{S}
r</em>{it}(k)r_{tj}(l) \ r_{ij}(k+1) = <em>{t=1}^{S} r</em>{it}(k)
p_{tj}</p>
<p>$$</p>
<h2 id="definition-2">Definition:</h2>
<p><span class="math inline"><em>j</em></span> is
<strong>accessible</strong> from <span
class="math inline"><em>i</em></span> if</p>
<p>$$ (j A(i), i j) \ \ &gt; 0 \ \ <em>{k=0}^{} r</em>{ij}(k) &gt; 0 \ \
i j \ </p>
<p>$$</p>
<h2 id="definition-3">Definition:</h2>
<p><span class="math inline"><em>i</em></span> and <span
class="math inline"><em>j</em></span> from <span
class="math inline"><em>S</em></span> are <strong>commuting
states</strong> <span class="math inline">(<em>i</em>â†”ï¸<em>j</em>)</span>
iff <span class="math inline"><em>i</em>â€„â†’â€„<em>j</em></span> and <span
class="math inline"><em>j</em>â€„â†’â€„<em>i</em></span>.</p>
<h2 id="lemma">Lemma:</h2>
<p><span class="math inline">â†”ï¸</span> is an equivalence relation.</p>
<h3 id="proof-1">Proof</h3>
<p>We need to show that it satisfies reflexivity, symmetry and
transitivity.</p>
<ol type="1">
<li><span class="math inline"><em>i</em>â€„â†”ï¸â€„<em>i</em></span> which means
<span class="math inline"><em>i</em>â€„â†’â€„<em>i</em></span> so <span
class="math inline"><em>r</em><sub><em>i</em><em>i</em></sub>(0)â€„=â€„1</span></li>
<li><span class="math inline"><em>i</em>â€„â†”ï¸â€„<em>j</em></span> iff <span
class="math inline"><em>j</em>â€„â†”ï¸â€„<em>i</em></span> by definition</li>
<li><span class="math inline"><em>i</em>â€„â†”ï¸â€„<em>j</em></span> and <span
class="math inline"><em>j</em>â€„â†”ï¸â€„<em>t</em></span> we want to show <span
class="math inline"><em>i</em>â€„â†”ï¸â€„<em>t</em></span>, but we know <span
class="math inline"><em>i</em>â€„â†’â€„<em>j</em>â€„â†’â€„<em>t</em></span> and
<span class="math inline"><em>t</em>â€„â†’â€„<em>j</em>â€„â†’â€„<em>i</em></span> so
we use these paths (or just shorten them by first intersection).</li>
</ol>
<p>$$ </p>
<p>$$</p>
<h2 id="definition-4">Definition:</h2>
<p>An <strong>equivalence class</strong> in a Markov chain is a set of
states that are commuting with each other. The set is maximal with its
property. In other words, no additional state from <span
class="math inline"><em>S</em></span> can be included in the set without
breaking the commuting property.</p>
<h2 id="definition-5">Definition:</h2>
<p><em>MC</em> is called <strong>irreducible</strong> if <span
class="math inline">â†”ï¸</span> has just <span class="math inline">1</span>
equivalence class. This is equivalent to that <span
class="math inline">âˆ€<em>i</em><em>j</em>â€„:â€„<em>i</em>â€„â†”ï¸â€„<em>j</em></span>.</p>
<p><em>Or by graph theory we can say that the transition graph is
strongly connected and when we compress these classes we get
DAG.</em></p>
<h2 id="definition-6">Definition:</h2>
<p><span class="math inline"><em>i</em>â€„âˆˆâ€„<em>S</em></span> is called
<strong>recurrent</strong> if <span
class="math inline">âˆ€<em>j</em>â€„âˆˆâ€„<em>A</em>(<em>i</em>)â€„:â€„<em>i</em>â€„âˆˆâ€„<em>A</em>(<em>j</em>)</span>
and <strong>transient</strong> otherwise.</p>
<h2 id="theorem-1">Theorem:</h2>
<p><span class="math inline"><em>i</em>â€„âˆˆâ€„<em>S</em></span> we define
<span class="math inline">$f_{ii} = \Pr[\exist t \geq 1 : X_{t} =i \vert
X_{0} = i]$</span> or by words <em>â€œprobability of going back to <span
class="math inline"><em>i</em></span>â€</em>. Then:</p>
<ul>
<li><span class="math inline"><em>i</em></span> is recurrent iff <span
class="math inline"><em>f</em><sub><em>i</em><em>i</em></sub>â€„=â€„1</span></li>
<li><span class="math inline"><em>i</em></span> is transient iff <span
class="math inline"><em>f</em><sub><em>i</em><em>i</em></sub>â€„&lt;â€„1</span></li>
</ul>
<h3 id="proof-2">Proof:</h3>
<ul>
<li><span class="math inline"><em>i</em></span> is transient iff <span
class="math inline">$\exist j \in A(i) : i \notin A(j)$</span></li>
<li>starting with <span
class="math inline"><em>X</em><sub>0</sub>â€„=â€„<em>i</em></span> the
probability <span
class="math inline">âˆƒ<em>t</em>â€„â‰¥â€„1â€„:â€„<em>X</em><sub><em>t</em></sub>â€„=â€„<em>j</em></span>
is <span class="math inline"><em>p</em>â€„&gt;â€„0</span> and <span
class="math inline">Prâ€†[going to <em>i</em> from
<em>j</em>]â€„=â€„0â€„â‡’â€„<em>f</em><sub><em>i</em><em>i</em></sub>â€„â‰¤â€„1â€…âˆ’â€…<em>p</em></span></li>
<li>And if <span class="math inline"><em>i</em></span> is recurrent then
<span
class="math inline"><em>f</em><sub><em>i</em><em>i</em></sub>â€„=â€„1</span>.</li>
</ul>
<p>$$ </p>
<p>$$</p>
<h2 id="definition-7">Definition:</h2>
<ul>
<li><span class="math inline"><em>i</em>â€„âˆˆâ€„<em>S</em></span> we define
<span class="math inline"><em>V</em><sub><em>i</em></sub></span> as
number of visits to <span class="math inline"><em>i</em></span> or
written as <span
class="math inline">|{<em>t</em>:<em>X</em><sub><em>t</em></sub>=<em>i</em>}|</span></li>
<li><span
class="math inline"><em>V</em><sub><em>i</em></sub>â€„âˆˆâ€„â„•â€…âˆªâ€…{âˆ}</span> so
it is a random variable defined by <span
class="math inline"><em>X</em><sub>0</sub>,â€†<em>X</em><sub>1</sub>,â€†â€¦</span></li>
</ul>
<h2 id="theorem-2">Theorem</h2>
<ul>
<li><span class="math inline"><em>i</em></span> is recurrent <span
class="math inline">â€„â‡’â€„Prâ€†[<em>V</em><sub><em>i</em></sub>=âˆ|<em>X</em><sub>0</sub>=<em>i</em>]â€„=â€„1</span></li>
<li><span class="math inline"><em>i</em></span> is transient <span
class="math inline">â€„â‡’â€„(<em>V</em><sub><em>i</em></sub>|<em>X</em><sub>0</sub>=<em>i</em>)â€„âˆ¼â€„Geom(1âˆ’<em>f</em><sub><em>i</em><em>i</em></sub>)</span>,
where <span
class="math inline">(1âˆ’<em>f</em><sub><em>i</em><em>i</em></sub>)</span>
is called as <em>escape probability</em>.</li>
</ul>
<h1 id="steady-state">Steady State</h1>
<h2 id="definition-8">Definition:</h2>
<p>Let <span class="math inline"><em>Ï€</em></span> be a distribution on
<span class="math inline"><em>S</em></span> such that <span
class="math inline">(<em>Ï€</em><sub>1</sub>+<em>Ï€</em><sub>2</sub>+â€¦+<em>Ï€</em><sub><em>S</em></sub>=1,<em>Ï€</em><sub><em>i</em></sub>&gt;0)</span>.
Then <span class="math inline"><em>Ï€</em></span> is
<strong>stationary</strong> distribution if <span
class="math inline"><em>Ï€</em><em>P</em>â€„=â€„<em>Ï€</em></span>. Or can be
written as <span
class="math inline">[<em>Ï€</em>=(<em>Ï€</em><sub>1</sub>,<em>Ï€</em><sub>2</sub>,â€¦)|âˆ€<em>j</em><em>Ï€</em><sub><em>j</em></sub>=âˆ‘<sub><em>i</em>â€„âˆˆâ€„<em>S</em></sub><em>Ï€</em><sub><em>i</em></sub><em>p</em><sub><em>i</em><em>j</em></sub>]</span>
for <em>MC</em> with transition matrix <span
class="math inline"><em>P</em></span>.</p>
<h2 id="observation-1">Observation</h2>
<p>If <span
class="math inline"><em>Ï€</em><sup>(0)</sup>â€„=â€„<em>Ï€</em></span> and
<span class="math inline"><em>Ï€</em></span> is stationary then <span
class="math inline"><em>Ï€</em><sup>(1)</sup>â€„=â€„<em>Ï€</em></span> and
<span
class="math inline">âˆ€<em>k</em>â€„:â€„<em>Ï€</em><sup>(<em>k</em>)</sup>â€„=â€„<em>Ï€</em></span>.</p>
<h2 id="definition-9">Definition:</h2>
<ul>
<li><span class="math inline"><em>s</em>â€„âˆˆâ€„<em>S</em></span> is
<strong>periodic</strong> if <span
class="math inline">âˆƒ<em>Î”</em>â€„â‰¥â€„2</span> integer such that <span
class="math inline">Prâ€†[<em>X</em><sub><em>t</em></sub>=<em>s</em>|<em>X</em><sub>0</sub>=<em>s</em>]â€„&gt;â€„0â€„â‡’â€„<em>Î”</em>|<em>t</em></span>.</li>
<li><em>MC</em> is periodic if all its states are periodic, otherwise it
is <em>aperiodic</em>.</li>
</ul>
<h2 id="theorem-3">Theorem</h2>
<p><span
class="math inline">(<em>X</em><sub><em>t</em></sub>)<sub><em>t</em>â€„=â€„0</sub><sup>âˆ</sup></span>
is a <em>MC</em> that is <em>irreducible</em>, <em>aperiodic</em> and
<span class="math inline">|<em>S</em>|â€„&lt;â€„âˆ</span>. Then <span
class="math inline">$\exist \pi$</span> that is a stationary
distribution and</p>
<ul>
<li><span
class="math inline">âˆ€<em>j</em>âˆ€<em>i</em>lim<sub><em>k</em>â€„â†’â€„âˆ</sub><em>r</em><sub><em>i</em><em>j</em></sub>(<em>k</em>)â€„=â€„<em>Ï€</em><sub><em>j</em></sub></span></li>
<li><span class="math inline"><em>Ï€</em></span> is a unique solution
to</li>
</ul>
<p>$$ P = \ = 1 % vector of 1s % TODO: I think this is wrong. Pi is a
probability distribution, so if pi_j is 1, then all other indexes must
be 0.</p>
<p>$$</p>
<h1 id="absorption-probability">Absorption probability</h1>
<h2 id="definition-10">Definition:</h2>
<p>Absorption states are such states, that the probability of staying in
the same state is <span class="math inline">1</span>. Or it is <span
class="math inline">{<em>s</em>â€„âˆˆâ€„<em>S</em>â€„:â€„<em>p</em><sub><em>s</em><em>s</em></sub>â€„=â€„1}</span>.</p>
<h2 id="lemma-probability-of-absorption">Lemma (Probability of
Absorption)</h2>
<p>Assume a <em>MC</em> with absorbing state <span
class="math inline">0</span> (and some move). Put</p>
<p>$$ a_{i} = i S</p>
<p>$$</p>
<p>Then <span
class="math inline">(<em>a</em><sub><em>i</em></sub>)</span> are the
unique solution to:</p>
$$
<p>$$</p>
<h3 id="proof-3">Proof</h3>
<ul>
<li><span class="math inline"><em>a</em><sub>0</sub>â€„=â€„1</span> and
<span class="math inline"><em>a</em><sub><em>i</em></sub>â€„=â€„0</span> if
<span class="math inline"><em>i</em>â€„â‰ â€„0</span> and absorbing is easy
observation</li>
<li>lets assume <span class="math inline"><em>i</em></span> is not
absorbing then</li>
</ul>
<p>$$ a_{i} = = \ = <em>{j S} = \ = </em>{j S} p_{ij} = \ =<em>{j S}
p</em>{ij} a_{j} \ </p>
<p>$$</p>
<h1 id="mean-time-to-absorption">Mean time to absorption</h1>
<p><span class="math inline"><em>A</em>â€„âŠ†â€„<em>S</em></span> is set of
all absorption states. <span
class="math inline"><em>T</em>â€„=â€„minâ€†{<em>t</em>â€„â‰¥â€„0|<em>X</em><sub><em>t</em></sub>â€„âˆˆâ€„<em>A</em>}</span>
is <em>absorption time</em> and random variable. Then we define <span
class="math inline"><em>Î¼</em><sub><em>i</em></sub>â€„=â€„ğ”¼[<em>T</em>|<em>X</em><sub>0</sub>=<em>i</em>]</span>.</p>
<h2 id="theorem-4">Theorem:</h2>
<p><span
class="math inline">(<em>Î¼</em><sub><em>i</em></sub>)<sub><em>i</em>â€„âˆˆâ€„<em>S</em></sub></span>
is the unique solution to:</p>
$$
<p>$$</p>
<h1 id="sat">SAT</h1>
<p>Problem where there is given a Boolean formula and we have to say if
it is satisfiable.</p>
<h2 id="sat-polynomial">2-SAT (<em>polynomial</em>)</h2>
<p>Special case of <em>SAT</em> where all clauses have at most <span
class="math inline">2</span> literals.</p>
<h3 id="algorithm-for-2-sat">Algorithm for 2-SAT</h3>
<ol type="1">
<li>Start with any assignment <span
class="math inline">(<em>x</em><sub>1</sub>=<em>x</em><sub>2</sub>=â€¦=<em>x</em><sub><em>n</em></sub>=<em>F</em>)</span></li>
<li>Repeat up to <span
class="math inline">2<em>m</em><em>n</em><sup>2</sup></span> times
(<span class="math inline"><em>n</em></span> is the number of variables
and <span class="math inline"><em>m</em></span> is an arbitrary
parameter)
<ul>
<li>if <span class="math inline"><em>Ï†</em></span> is satisfiable return
â€œYESâ€</li>
<li>otherwise, choose any clause that is not satisfied and randomly
change one of its variables <span class="math inline">(*)</span></li>
</ul></li>
<li>Return â€œNOâ€</li>
</ol>
<p><span class="math inline">$Pr[\text{incorrectly saying no}] \leq
\frac{1}{2m}$</span> which can be proved by Markov inequality.</p>
<p><span class="math inline">$Pr[\text{incorrectly saying no}] \leq
\frac{1}{2^m}$</span> using iterative Markov inequality.</p>
<h2 id="sat-1">3-SAT</h2>
<h3 id="algortihm-for-3-sat">Algortihm for 3-SAT</h3>
<ul>
<li>Repeat for <span class="math inline">â€„â‰¤â€„<em>m</em></span> times
<ul>
<li>Repeat for <span
class="math inline">â€„â‰¤â€„3<sup><em>n</em>/2</sup></span> times
<ul>
<li>randomly initialize the variables</li>
<li>if <span class="math inline"><em>Ï†</em></span> is satisfiable return
â€œYESâ€</li>
<li>otherwise, choose any clause that is not satisfied and randomly
change one of its variables</li>
</ul></li>
</ul></li>
</ul>
<p>Running time of this algortihm is exponential in <span
class="math inline"><em>n</em></span>.</p>
<p><span class="math inline">$P[\text{failure}] \leq
\frac{1}{2^m}$</span></p>
<p>By using a better algorithm we can get the exponential part to be
<span class="math inline">$\frac{4^n}{3}$</span>.</p>
<p>The idea behind these algorithms is that we are using a <em>random
walk</em> on the space of all possible assignments. This is a <em>Markov
chain</em>. So we can easily calculate the probability of getting to the
absorbing state and the mean time to get there.</p>
<h2 id="what-is-probability">What is probability?</h2>
<p>We may look at probability from different angles.</p>
<ol type="1">
<li>Math concepts.
<ul>
<li>axioms, examples <span class="math inline">$\frac{\#
\text{good}}{\#\text{all}}$</span>, theorems â€¦</li>
<li>interesting/useful probabilistic method as â€œto show <span
class="math inline"><em>A</em>â€„â‰ â€„0</span> we show <span
class="math inline">Prâ€†[<em>A</em>]â€„&gt;â€„0</span>â€, lower bounds for
Ramsey number</li>
</ul></li>
<li>Description of real world. Question: <em>Does Nature play dice?</em>
<ul>
<li>YES, if quantum theory is right so called <em>true
randomness</em></li>
<li>imprecise measurements so called <em>pseudo randomness</em></li>
</ul></li>
</ol>
<p>Then we we have two possible approaches.</p>
<ol type="1">
<li><strong>Frequntistâ€™s approach</strong> <span
class="math inline">$\frac{\text{\# good}}{\text{\# all}}$</span></li>
<li><strong>Bayesian approach</strong> as subjective probability, so we
are counting with all possible universes and what is the probability
this will happen in our universe.</li>
</ol>
<h1 id="bayesian-statistics">Bayesian statistics</h1>
<ol type="1">
<li><span class="math inline"><em>Î˜</em></span> is random variable
describing some quantity of interest</li>
<li><span
class="math inline"><em>X</em>â€„=â€„(<em>X</em><sub>1</sub>,â€¦,<em>X</em><sub><em>n</em></sub>)</span>
measurements</li>
</ol>
<p><em>Note: In Frequentistâ€™s approach <span
class="math inline"><em>Î˜</em></span> does not exist we have <span
class="math inline"><em>Ï‘</em></span> as unknown fixed
parameter.</em></p>
<div data-align="center">
<table>
<thead>
<tr class="header">
<th>variable</th>
<th>PMF</th>
<th>PDF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span
class="math inline"><em>p</em><sub><em>Î˜</em></sub></span></td>
<td><span
class="math inline"><em>f</em><sub><em>Î˜</em></sub></span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span
class="math inline"><em>p</em><sub><em>X</em></sub></span></td>
<td><span
class="math inline"><em>f</em><sub><em>X</em></sub></span></td>
</tr>
</tbody>
</table>
</div>
<h2 id="bayes-theorem">Bayes theorem</h2>
<p>$$ = </p>
<p>$$</p>
<p>Where <span
class="math inline">Prâ€†[<em>A</em>],â€†Prâ€†[<em>B</em>]â€„&gt;â€„0</span>. We
will consider <span class="math inline"><em>B</em></span> as <span
class="math inline"><em>Î˜</em>â€„=â€„<em>Ï‘</em></span> and <span
class="math inline"><em>A</em></span> as measurements <span
class="math inline"><em>X</em>â€„=â€„<em>x</em></span>. Now we get:</p>
<p>$$ = </p>
<p>$$</p>
<p>Where <span
class="math inline">Prâ€†[<em>Î˜</em>=<em>Ï‘</em>|<em>X</em>=<em>x</em>]</span>
is called <strong>posterior</strong> and it is the probability after
some measurements. <span
class="math inline">Prâ€†[<em>Î˜</em>=<em>Ï‘</em>]</span> is called
<strong>prior</strong> as an probability and <span
class="math inline">Prâ€†[<em>X</em>=<em>x</em>|<em>Î˜</em>=<em>Ï‘</em>]</span>
is our current model of the world (<em>likelihood</em>).</p>
<h2 id="bayes-theorem-using-pmf">Bayes theorem using PMF</h2>
<p>$$ p_{X} (x) = = c p_{}() p_{X }(x )</p>
<p>$$</p>
<p>For some konstant <span class="math inline"><em>c</em></span>.</p>
<h3 id="what-do-we-want">What do we want?</h3>
<ol type="1">
<li>Point estimate for <span class="math inline"><em>Î˜</em></span>.</li>
<li>Interval estimate for <span
class="math inline"><em>Î˜</em></span>.</li>
<li>Hypothesis testing.</li>
</ol>
<p>For <em>interval estimate</em> we have given <span
class="math inline"><em>X</em></span> and want to find <span
class="math inline">[<em>a</em>,<em>b</em>]</span> as <span
class="math inline">(<em>a</em>=<em>a</em>(<em>X</em>),<em>b</em>=<em>b</em>(<em>X</em>))</span>.
<span
class="math inline">Prâ€†[<em>a</em>(<em>x</em>)&lt;<em>Î˜</em>&lt;<em>b</em>(<em>X</em>)|<em>X</em>=<em>x</em>]â€„â‰¥â€„1â€…âˆ’â€…<em>Î±</em></span>.
Perhaps <span class="math inline">$\Pr[\Theta &lt; a(x) \vert X =x ] =
\frac{\alpha}{2}$</span> and <span class="math inline">$\Pr[\Theta &gt;
b(x) \vert X =x ] = \frac{\alpha}{2}$</span>.</p>
<p>For <em>point estimate</em> we have two approaches.</p>
<h4 id="map-as-for-maximum-aposteriori-probability">1) MAP as for
maximum aposteriori probability</h4>
<p>$$ = <em>{} p</em>{X} (x)</p>
<p>$$</p>
<p>If <span class="math inline"><em>X</em>â€„=â€„<em>x</em></span> what is
the most likely value?</p>
<h4 id="lms-as-for-least-mean-square">2) LMS as for least mean
square</h4>
$$
<p>$$</p>
<h2 id="naive-bayes">Naive Bayes</h2>
<p>By the Bayesian statistics we get for <span
class="math inline"><em>X</em><sub>1</sub></span>:</p>
<p>$$ p_{X_{1}} (x_{1}) = </p>
<p>$$</p>
<p>But what if we have <span class="math inline"><em>n</em></span>
measurements to consider. Then we have <span
class="math inline">Prâ€†[<em>Î˜</em>=<em>Ï‘</em>|<em>X</em><sub>1</sub>=<em>x</em><sub>1</sub>,<em>X</em><sub>2</sub>=<em>x</em><sub>2</sub>,â€¦]</span>
which can be computed by naive Bayes as:</p>
<p>$$ = </p>
<p>$$</p>
<p>Also <span
class="math inline"><em>p</em><sub><em>X</em>|<em>Î˜</em></sub>(<em>x</em><sub><em>i</em></sub>,â€¦,<em>x</em><sub>1</sub>|<em>Î˜</em>=<em>Ï‘</em>)</span>
is <em>joint PMF</em> and we assume conditional independence.</p>
<h2 id="bayes-theorem-using-pdf">Bayes theorem using PDF</h2>
<p>As for PMF we have Bayesian statistics for PDF.</p>
<p>$$ f_{X}(x) = </p>
<p>$$</p>
<h1 id="beta-distribution">Beta distribution</h1>
<p>To see some nice properties of Bayesian theorem we will look into one
new distribution. We will have <span
class="math inline"><em>Î±</em>,â€†<em>Î²</em>â€„â‰¥â€„1</span> and <span
class="math inline"><em>Ï‘</em>â€„âˆˆâ€„[0,1]</span>. Then</p>
<p>$$ f_{} () = </p>
<p>$$</p>
<p>Where <span class="math inline">$\Beta(\alpha, \beta)$</span> is
called <strong>beta function</strong> and for all <span
class="math inline"><em>Î±</em>,â€†<em>Î²</em></span> it is a constant. For
example the beta function for <span
class="math inline">$\Beta(1,1)$</span> is equal to <span
class="math inline">1</span> from <span class="math inline">[0,1]</span>
and <span class="math inline">0</span> otherwise. And <span
class="math inline">$\Beta(1,2) = \frac{1}{2}$</span>. It serves as a
normalizing constant for the beta distribution.</p>
<ul>
<li>Firstly the maximum is at <span class="math inline">$\frac{\alpha
-1}{\alpha + \beta - 2}$</span> which is the <strong>mode</strong>
(<em>cz: modus</em>).</li>
<li>Secondly</li>
</ul>
<p>$$ (, ) = = </p>
<p>$$</p>
<ul>
<li>Lastly <span class="math inline">$\mathbb{E} [\Theta] =
\frac{\alpha}{\alpha + \beta}$</span> which is the
<strong>mean</strong>.</li>
</ul>
<p>Now we will look into the Bayesian theorem using Beta distribution as
a prior and Binomial distribution as a likelihood.</p>
<p>$$ p_{X }(k ) = ^{k} (1-)^{n-k}</p>
<p>$$</p>
<p>$$ f_{X}(x) = c_{1} ^{- 1}(1 - )^{- 1} c_{2} ^{x} (1- )^{1-x} c_{3}
=</p>
<p>$$</p>
<p>Where <span
class="math inline"><em>c</em><sub>1</sub>,â€†<em>c</em><sub>2</sub>,â€†<em>c</em><sub>3</sub></span>
do not depend on <span class="math inline"><em>Ï‘</em></span> and are
some constants.</p>
<p>$$ = c_{4} ^{+ k -1}(1 - )^{+ n - k -1}</p>
<p>$$</p>
<p>And that is some other Beta distribution with <span
class="math inline"><em>Î±</em>â€²â€„=â€„<em>Î±</em>â€…+â€…<em>x</em></span> and
<span
class="math inline"><em>Î²</em>â€²â€„=â€„<em>Î²</em>â€…+â€…<em>n</em>â€…âˆ’â€…<em>x</em></span>.
And also we have these point estimates:</p>
<ol type="1">
<li>MAP <span class="math inline">$\hat{\vartheta} = \frac{x}{n}$</span>
which is same as likelihood.</li>
<li>LMS <span class="math inline">$\hat{\vartheta} = \mathbb{E}(\Theta
\vert X = x) = \frac{x+1}{n+2}$</span></li>
</ol>
<h2 id="normal-random-variable">Normal random variable</h2>
<p>Also we can look at Bayesian theorem with normal variables. <em>Note:
This doesnâ€™t seem so interesting and useful, since it is only
computation and nothing else.</em></p>
<!-- TODO: have a look at this if you have any time left -->
<h1 id="conditional-expectation">Conditional expectation</h1>
<p>Firstly we will remind how expectation is defined. <span
class="math inline">ğ”¼[<em>Y</em>]â€„=â€„âˆ‘<sub><em>y</em>â€„âˆˆâ€„Img(<em>Y</em>)</sub><em>y</em>Prâ€†[<em>Y</em>=<em>y</em>]</span>
if <span class="math inline"><em>Y</em></span> is discrete or <span
class="math inline">â€„=â€„âˆ«<sub>âˆ’âˆ</sub><sup>âˆ</sup><em>y</em><em>f</em><sub><em>Y</em></sub>(<em>y</em>)
d<em>y</em></span> if <span class="math inline"><em>y</em></span> is
continuous. Now we will show how conditional expectation is defined.</p>
$$
<p>$$</p>
<p>Now if we have <span
class="math inline"><em>X</em>,â€†<em>Y</em></span> discrete random
variables and <span class="math inline"><em>x</em>â€„âˆˆâ€„â„</span>, then:</p>
<p>$$ [Y X = x] =: g(x)</p>
<p>$$</p>
<p>So <span class="math inline"><em>g</em></span> is a function <span
class="math inline">â„â€„â†’â€„â„</span>. Then</p>
<p>$$ [Y X] =: g(X)</p>
<p>$$</p>
<p>So we have two functions <span
class="math inline"><em>Î©</em>â†’<sup><em>X</em></sup>â„â†’<sup><em>g</em></sup>â„</span>.
Now we will show one property which is called <strong>Law of Iterated
Expectation</strong>.</p>
<p>$$ [[Y X]] =^{} [g(X)] =^{} <em>{x (X)}g(x) = \ =^{} </em>{x (X)} [Y
X =x] = [Y]</p>
<p>$$</p>
<p>Where the last equivalence is by the Law of total Expectation. So by
this we get <span
class="math inline">ğ”¼[ğ”¼[<em>Y</em>|<em>X</em>]]â€„=â€„ğ”¼[<em>Y</em>]</span>
if <span class="math inline">ğ”¼[<em>Y</em>]â€„&lt;â€„âˆ</span>.</p>
<p>Now, we will use a similar approach to find an alternative definition
of variance.</p>
<p>Let $Y = - $ where <span class="math inline"><em>YÌ‚</em></span> and
<span class="math inline"><em>YÌƒ</em></span> are statistically
independent and <span
class="math inline"><em>v</em><em>a</em><em>r</em>(<em>YÌƒ</em>)â€„=â€„ğ”¼[<em>YÌƒ</em><sup>2</sup>]</span></p>
<p>$$</p>
<p>var(Y) = var() + var() - 2cov(, )</p>
<p>$$</p>
<p>From the property of the statistical independence we get <span
class="math inline"><em>c</em><em>o</em><em>v</em>(<em>YÌ‚</em>,<em>YÌƒ</em>)â€„=â€„0</span>.</p>
<p>$$ [(Y - E[YX])^2 X] = var[Y X] =: h(X)</p>
<p>$$</p>
<h3 id="law-of-iterated-variance">Law of iterated variance</h3>
<p>$$ [Y] = [[Y X]] + [[Y X]]</p>
<p>$$</p>
<p>Or it is called an <strong>Eveâ€™s rule</strong> (<em>as E for expected
value and V for variance</em>). We may simulate it by saying that the
first part of the sum is expected value of variance within one group and
the second part is inter group variance. <em>This is also partly from
the example that was sadly omitted.</em></p>
<p>Next we can show that Least Mean Square is iff condition expectation.
That is for given <span class="math inline"><em>Y</em></span> what is
the value of <span class="math inline"><em>y</em></span> that minimizes
<span
class="math inline">ğ”¼[<em>Y</em>âˆ’<em>y</em>]<sup>2</sup></span>?</p>
<p>$$ [Y - y]^{2} = [Y^{2}] - 2y[Y] + y^{2} = f(y) \ fâ€™(y) = - 2 [Y] +
2y = 0 y = [Y]</p>
<p>$$</p>
<p>Now we want for all <span class="math inline"><em>x</em></span> find
<span class="math inline"><em>y</em>â€„=â€„<em>y</em>(<em>x</em>)</span>
such that <span
class="math inline">ğ”¼[(<em>Y</em>âˆ’<em>y</em>(<em>x</em>))<sup>2</sup>|<em>X</em>=<em>x</em>]</span>
is minimized. We can show by similar calculation that <span
class="math inline"><em>y</em>(<em>x</em>)â€„=â€„ğ”¼[<em>Y</em>|<em>X</em>=<em>x</em>]</span>.
And our best (in the LMS sense) estimation is <span
class="math inline"><em>YÌ‚</em>â€„=â€„ğ”¼[<em>Y</em>|<em>X</em>]</span>.</p>
<h1 id="stochastic-processes">Stochastic processes</h1>
<p>Stochastic process is a sequence of random variables <span
class="math inline"><em>X</em><sub>1</sub>,â€†<em>X</em><sub>2</sub>,â€†<em>X</em><sub>3</sub>,â€†â€¦</span>.
We will show that there exist many of them.</p>
<ul>
<li>Markov chain (<em>+ extra condition</em>)</li>
<li>Wiener process
<ul>
<li>Browner motion</li>
<li>Stock prices</li>
<li>Limit version of RN</li>
</ul></li>
<li><strong>Arrival times</strong> or alternatively <strong>waiting for
success</strong>.</li>
</ul>
<p>We will be looking at the last type of the processes.</p>
<h2 id="bernoulli-process-denoted-as-textbpp">Bernoulli process
(<em>denoted as</em> <span
class="math inline">Bp(<em>p</em>)</span>)</h2>
<p>That is we have <span
class="math inline"><em>X</em><sub>1</sub>,â€†<em>X</em><sub>2</sub>,â€†â€¦</span>
iid and each one of them is <span
class="math inline"><em>X</em><sub><em>i</em></sub>â€„âˆ¼â€„Ber(<em>p</em>)</span>
so with probability <span class="math inline"><em>p</em></span> it is
<span class="math inline">1</span> and <span
class="math inline">0</span> with probability <span
class="math inline">1â€…âˆ’â€…<em>p</em></span>.</p>
<h3 id="observations">Observations</h3>
<ul>
<li><span
class="math inline"><em>X</em><sub><em>n</em></sub>,â€†<em>X</em><sub><em>n</em>â€…+â€…1</sub>,â€†â€¦</span>
is also <span class="math inline">Bp(<em>p</em>)</span></li>
<li><span
class="math inline"><em>X</em><sub><em>N</em></sub>,â€†<em>X</em><sub><em>N</em>â€…+â€…1</sub>,â€†â€¦</span>
is also <span class="math inline">Bp(<em>p</em>)</span>
<ul>
<li>with <span class="math inline"><em>N</em></span> a random variable
dependent only on the past</li>
</ul></li>
</ul>
<p>Then we will define <span
class="math inline"><em>T</em>â€„=â€„minâ€†{<em>t</em>â€„:â€„<em>X</em><sub><em>t</em></sub>â€„=â€„1}</span>
or by words the time of the first success / arrival. And we can easily
see that <span class="math inline"><em>T</em>â€„âˆ¼â€„Geom(<em>p</em>)</span>
so <span class="math inline">$\mathbb{E}[T] = \frac{1}{p}$</span> and
<span class="math inline">$\text{var} [T] =
\frac{1-p}{p^{2}}$</span>.</p>
<p>Now we will try to generalize this by <span
class="math inline"><em>T</em><sub><em>k</em></sub></span> as the time
of the <span class="math inline"><em>k</em></span>-th arrival. So <span
class="math inline"><em>T</em><sub>1</sub>â€„=â€„<em>T</em></span>. Or
written as <span
class="math inline"><em>T</em><sub><em>k</em></sub>â€„=â€„minâ€†{<em>t</em>â€„:â€„<em>X</em><sub>1</sub>â€…+â€…<em>X</em><sub>2</sub>â€…+â€…â€¦â€…+â€…<em>X</em><sub><em>t</em></sub>â€„=â€„<em>k</em>}</span>.</p>
<p>Other interesting variable is the <span
class="math inline"><em>k</em></span>-th waiting time (inter arrival)
and it will be denoted as <span
class="math inline"><em>L</em><sub><em>k</em></sub></span>. To describe
this variable it is the time between <span
class="math inline"><em>k</em>â€…âˆ’â€…1</span> arrival and <span
class="math inline"><em>k</em></span>-th arrival. Then it follows</p>
<p>$$ L_{k} = T_{k} - T_{k-1} T_{0} = 0 \ L_{k} L_{1} = T L_{k} (p)</p>
<p>$$</p>
<p>And all <span
class="math inline"><em>L</em><sub><em>i</em></sub></span> are
independent. From the other way we can define <span
class="math inline"><em>T</em><sub><em>k</em></sub></span> as the sum
<span class="math inline">$\sum_{i=1}^{k} = L_{i}$</span>. So we can
then get expected value and variance.</p>
<p>$$ [T_{k}] = [L_{1}] + [L_{2}] + + [L_{k}] = </p>
<p>$$</p>
<p>$$ [T_{k}] = [L_{1}] + [L_{2}] + + [L_{k}] = k </p>
<p>$$</p>
<p>How could we compute <span
class="math inline">Prâ€†[<em>T</em><sub><em>k</em></sub>=<em>t</em>]â€„=â€„?</span>
Easily we can compute this by convolution formula <span
class="math inline">$\binom{t-1}{k-1} p^{k}(1-p)^{t-k}$</span>.</p>
<p>Lastly we define <span
class="math inline"><em>N</em><sub><em>t</em></sub></span> as the sum
<span
class="math inline"><em>X</em><sub>1</sub>â€…+â€…<em>X</em><sub>2</sub>â€…+â€…â€¦â€…+â€…<em>X</em><sub><em>t</em></sub></span>
which is the number of successes till the time <span
class="math inline"><em>t</em></span>. And <span
class="math inline"><em>N</em><sub><em>t</em></sub>â€„âˆ¼â€„Bin(<em>t</em>,<em>p</em>)</span>.
So <span
class="math inline">ğ”¼[<em>N</em><sub><em>t</em></sub>]â€„=â€„<em>t</em><em>p</em></span>
and <span
class="math inline">var[<em>N</em><sub><em>t</em></sub>]â€„=â€„<em>t</em><em>p</em>(1âˆ’<em>p</em>)</span>.</p>
<h3 id="alternative-definition">Alternative definition</h3>
<p>We can define Bernoulli process by different definition. First we
will define <span
class="math inline"><em>L</em><sub>1</sub>,â€†<em>L</em><sub>2</sub>,â€†â€¦</span>
as iid <span class="math inline">â€„âˆ¼â€„Geom(<em>p</em>)</span> and then
<span class="math inline">$T_{k} = \sum_{i=1}^{k} L_{i}$</span>. And
<span class="math inline"><em>X</em><sub><em>i</em></sub></span> is
<span class="math inline">1</span> if <span
class="math inline"><em>T</em><sub><em>k</em></sub>â€„=â€„<em>i</em></span>
for some <span class="math inline"><em>k</em></span> or <span
class="math inline">0</span> otherwise. Then <span
class="math inline">(<em>X</em><sub><em>i</em></sub>)<sub><em>i</em></sub></span>
is <span class="math inline">Bp(<em>p</em>)</span>.</p>
<h3 id="merging-of-bernoulli-process">Merging of Bernoulli process</h3>
<p>We will have two processes which are independent.</p>
$$
<p>$$</p>
<p>Then the merge is <span
class="math inline"><em>Z</em><sub><em>i</em></sub>â€„=â€„<em>X</em><sub><em>i</em></sub>
or <em>Y</em><sub><em>i</em></sub></span>. Properly it is</p>
$$
<p>$$</p>
<h3 id="splitting-bernoulli-process">Splitting Bernoulli process</h3>
<p>We can also split one Bernoulli process. Firstly we got</p>
$$
<p>$$</p>
<p>If <span
class="math inline"><em>Z</em><sub><em>i</em></sub>â€„=â€„1</span> then
<span class="math inline"><em>X</em><sub><em>i</em></sub>â€„=â€„1</span>
with probability <span class="math inline"><em>Î±</em></span> and <span
class="math inline">0</span> with probability <span
class="math inline">(1âˆ’<em>Î±</em>)</span> and if <span
class="math inline"><em>Z</em><sub><em>i</em></sub>â€„=â€„0</span> then
<span class="math inline"><em>X</em><sub><em>i</em></sub>â€„=â€„0</span>. By
this construction we get new Bernoulli process.</p>
$$
<p>$$</p>
<h2 id="poisson-process-denoted-as-textpplambda">Poisson process
(<em>denoted as</em> <span
class="math inline">Pp(<em>Î»</em>)</span>)</h2>
<p>As we defined Bernoulli process we also can define Poisson process
which can be described as a continuous approximation of <span
class="math inline">Bp(<em>p</em>)</span>. Now the arrival times are
real numbers.</p>
<h3 id="definition-11">Definition:</h3>
<ol type="1">
<li>For any interval of length <span
class="math inline"><em>Ï„</em></span> probability of <span
class="math inline"><em>k</em></span> arrivals is the same. Denoted as
<span class="math inline"><em>P</em>(<em>k</em>,<em>Ï„</em>)</span>.</li>
<li>Number of arrivals in <span
class="math inline">[<em>a</em>,<em>b</em>]</span> is independent of
number in <span class="math inline">[0,â€†<em>a</em>)</span>.</li>
<li><span
class="math inline"><em>P</em>(0,<em>Ï„</em>)â€„=â€„1â€…âˆ’â€…<em>Î»</em><em>Ï„</em>â€…+â€…<em>o</em>(1)</span>,
<span
class="math inline"><em>P</em>(1,<em>Ï„</em>)â€„=â€„<em>Î»</em><em>Ï„</em>â€…+â€…<em>o</em>(1)</span>,
<span
class="math inline"><em>P</em>(<em>k</em>,<em>Ï„</em>)â€„=â€„<em>o</em>(1)</span>.
for <span class="math inline"><em>k</em>â€„â‰¥â€„2</span> where <span
class="math inline"><em>o</em>(1)</span> is something that goes to
zero</li>
</ol>
<!-- TODO: What is the small o?? -->
<p>Then the sequence <span
class="math inline"><em>T</em><sub>1</sub>.<em>T</em><sub>2</sub>,â€†<em>T</em><sub>3</sub>,â€†â€¦</span>
is <span class="math inline">Pp(<em>Î»</em>)</span> where Ts are the
arrival times.</p>
<!-- $Pr[N_t = k] = P(k, t) = $ -->
<p>As in Bernoulli process we have <span
class="math inline"><em>T</em><sub><em>k</em></sub></span> as the time
of <span class="math inline"><em>k</em></span>-th arrival. Then <span
class="math inline"><em>N</em><sub><em>T</em></sub></span> is the number
of arrivals in <span class="math inline">[0,<em>t</em>]</span> and <span
class="math inline"><em>N</em><sub><em>T</em></sub>â€„âˆ¼â€„Pois(<em>Î»</em><em>t</em>)</span>
so <span class="math inline">$P(k, t) = e^{-\lambda t} \frac{(\lambda
t)^{k}}{k!}$</span>.</p>
<p>We can show that by the following approximation:</p>
<p>$$ Pr[N_t = k] = P(k,t) P(1, ) = + o(1) Pr[N_t = k] = P(k, t) P[] =
Pr[Bin(l, ) = k] lim_{l } Bin(l, ) Pois(t)</p>
<p>$$</p>
<p>Then again <span
class="math inline"><em>L</em><sub><em>k</em></sub>â€„=â€„<em>T</em><sub><em>k</em></sub>â€…âˆ’â€…<em>T</em><sub><em>k</em>â€…âˆ’â€…1</sub></span>
so <span
class="math inline">Prâ€†[<em>L</em><sub><em>k</em></sub>â‰¥<em>t</em>]â€„=â€„Prâ€†[no
arrival in
[<em>T</em><sub><em>k</em>â€…âˆ’â€…1</sub>,<em>T</em><sub><em>k</em>â€…âˆ’â€…1</sub>+<em>t</em>]]</span>
and that is equal to <span
class="math inline"><em>P</em>(0,<em>t</em>)â€„=â€„<em>e</em><sup>âˆ’<em>Î»</em><em>t</em></sup></span>.
Next <span
class="math inline">Prâ€†[<em>L</em><sub><em>k</em></sub>â‰¤<em>t</em>]â€„=â€„1â€…âˆ’â€…<em>e</em><sup>âˆ’<em>Î»</em><em>t</em></sup>â€„â‡’â€„<em>L</em><sub><em>k</em></sub>â€„âˆ¼â€„Exp(<em>Î»</em>)</span>.</p>
<h3 id="alternative-description">Alternative description</h3>
<p>As in Bernoulli process we can define Poisson process the other way
around. We start with sequence of iid <span
class="math inline"><em>L</em><sub>1</sub>,â€†<em>L</em><sub>2</sub>,â€†â€¦â€„âˆ¼â€„Exp(<em>Î»</em>)</span>.
Then <span class="math inline"><em>T</em><sub><em>k</em></sub></span> is
the sum <span class="math inline">$T_{k} = \sum_{i=1}^{k} L_{i}$</span>.
And we also get the same <span
class="math inline"><em>N</em><sub><em>t</em></sub></span>.</p>
<h3 id="theorem-5">Theorem</h3>
<p>This also defines <span class="math inline">Pp(<em>Î»</em>)</span>. In
other words it satisfies all of the three properties.</p>
<p>Again as in Bp we can see that expected value of <span
class="math inline"><em>T</em><sub><em>k</em></sub></span> and variance
is the sum of expected values (resp. variances) of <span
class="math inline"><em>L</em><sub><em>i</em></sub></span> which are
<span class="math inline">$\frac{1}{\lambda}$</span> (resp. <span
class="math inline">$\frac{1}{\lambda^{2}}$</span>). By convolution we
get that</p>
<p>$$ f_{T_{k}} (t) = </p>
<p>$$</p>
<h3 id="splitting-of-pp">Splitting of Pp</h3>
<p>We have a <span class="math inline">Pp(<em>Î»</em>)</span> and each
one is split (1 or 0) with probability <span
class="math inline"><em>p</em></span> (resp. <span
class="math inline">1â€…âˆ’â€…<em>p</em></span>). And then we get two
processes <span class="math inline">Pp(<em>p</em><em>Î»</em>)</span> and
<span class="math inline">Pp((1âˆ’<em>p</em>)<em>Î»</em>)</span> and these
are independent. Two new processes have still the same properties but
with new <span class="math inline"><em>Î»</em>â€²</span>. To properly show
that this holds we need to show all the properties from the
definition.</p>
<!-- TODO -->
<p>$$</p>
<p>Pr[T_1 &gt; t] = Pr[T&gt;t Â &amp; Â Tâ€™ &gt; t] = </p>
<p>$$</p>
<p><em>Note: Proving independece is quite cumbersome. The proof is based
on an example from the lecture.</em></p>
<h3 id="merging-of-pp">Merging of Pp</h3>
<p>If we have two processes <span
class="math inline">Pp(<em>Î»</em>)</span> and <span
class="math inline">Pp(<em>Î»</em>â€²)</span> we can merge these to get
<span class="math inline">Pp(<em>Î»</em>+<em>Î»</em>â€²)</span>. Again to
properly show that this holds we must show that the <span
class="math inline">minâ€†</span> of two <span
class="math inline">Exp</span> distributions is again <span
class="math inline">Exp</span> distribution with the sum. Which is quite
easy since they are independent, then we get the product of exponent
functions which is the same as the sum of their exponents.</p>
<p>What if we look at the <span
class="math inline">Prâ€†[<em>T</em>âˆ’<em>t</em>&gt;<em>s</em>|<em>T</em>&gt;<em>t</em>]</span>
which by definition is <span class="math inline">$\frac{\Pr[T &gt; s+t
\land T &gt; t]}{\Pr [T &gt; t]}$</span> and that is equal to <span
class="math inline">$\frac{e^{-\lambda (s+t)}}{e^{-\lambda t}}
e^{-\lambda s}$</span> and we get the property that the Poisson process
is <strong>memory-less</strong> so it doesnâ€™t matter when we will start
measuring our data.</p>
<h1 id="balls-bins">Balls &amp; Bins</h1>
<p>This model is if we have <span class="math inline"><em>m</em></span>
balls and <span class="math inline"><em>n</em></span> bins and for each
ball we put it independently at random to one bin, where each bin has
the same probability.</p>
<p>One well known problem is <em>Birthday paradox</em> where we have
<span class="math inline"><em>k</em></span> people as balls and <span
class="math inline">365</span> days as bins. Then we are asking what is
the probability that one bin has at least 2 balls.</p>
<p>$$</p>
<p>Pr[] = 1 - Pr[] = \ = 1 - <em>{i=1}^{m-1} - </em>{i=1}^{m-1} e^{} = 1
- e^{}</p>
<p>$$</p>
<p>We also consider other properties, such as the expected number of
empty bins:</p>
<p>$$ Pr[ i ] = (1 - )^{m} e^{}</p>
<p>$$</p>
<p>$$</p>
<p>[ ] = n (1- )^{m} n e^{}</p>
<p>$$</p>
<h2 id="max-load-theorem">Max Load Theorem</h2>
<p>if <span class="math inline"><em>m</em>â€„=â€„<em>n</em></span> and are
big enough and <span class="math inline">$M = 3
\frac{ln(n)}{ln(ln(n))}$</span> then <span
class="math inline">$Pr[\text{max \# of balls in a bin} &gt; M] &lt;
\frac{1}{n}$</span></p>
<h2 id="proof-4">Proof</h2>
<!-- TODO: I don't think this is right. The proba of the Bin distribution should be smaller not greater than >= M balls -->
<p>$$</p>
<p>Pr[ #1 M ] Pr[Bin(n, ) = M] &lt; &lt; ()^M % The binomial
distribution is wrong in this case</p>
<p>$$</p>
<p>$$</p>
<p>Pr[ M ] Pr[ #1 M ] + + Pr[ #n M ] n ()^M</p>
<p>$$</p>
<p>Now we will show that this expression is smaller than <span
class="math inline">$\frac{1}{n}$</span>. In order to do that, we wil
take the logarithm of both sides and we get:</p>
<p>$$</p>
<p>2ln(n) + M(1-ln(M)) &lt; 0</p>
<p>$$</p>
<p>We will then substitute <span class="math inline"><em>M</em></span>
and show that the inequality holds.</p>
<p><span class="math display">â–¡</span></p>
<p>M balls and bins have multiple applications. We will use it for
hashing and sorting.</p>
<h2 id="bucket-sort-application">Bucket Sort Application</h2>
<p>We want to sort <span
class="math inline"><em>n</em>â€„=â€„2<sup><em>k</em></sup></span> numbers
from range <span class="math inline">[0,2<sup><em>l</em></sup>âˆ’1]</span>
where <span class="math inline"><em>l</em>â€„&gt;â€„<em>k</em></span>. The
numbers are uniformly random in this range.</p>
<h3 id="algorithm">Algorithm</h3>
<ol type="1">
<li>Put input <span class="math inline"><em>x</em></span> to a bucket
<span class="math inline"><em>b</em>(<em>x</em>)</span> where <span
class="math inline"><em>b</em>(<em>x</em>)</span> is a hash function of
<span class="math inline"><em>x</em></span> and bucket is a list</li>
<li>Sort each bucket (list) by a bubble sort in quadratic time</li>
<li>merge the buckets</li>
</ol>
<h3 id="time-analysis">Time Analysis</h3>
<p>Parts 1 and 3 are linear in <span
class="math inline"><em>n</em></span></p>
<p>For part 2, we will consider $X_i = # Bin(n, ) $. Then <span
class="math inline">ğ”¼
timeâ€„=â€„ğ”¼âˆ‘(<em>c</em><sub><em>i</em></sub><em>X</em><sub><em>i</em></sub><sup>2</sup>)</span>
Finally, we will use the definition of variance to show that the
expected time is $ &lt; 2cn$. Hence the whole algorithm has linear
expected time.</p>
<h2 id="hash-collisions-application">Hash Collisions Application</h2>
<p>We want to store <span class="math inline"><em>n</em></span> strings
and search fast. Using the max load theorem, we will show that max
running time with a big enough <span
class="math inline"><em>n</em></span> is $&lt; 3 $</p>
<h2 id="theorem-6">Theorem</h2>
<p>Distribution of <span
class="math inline"><em>X</em><sub>1</sub><sup>(<em>m</em>)</sup>,â€†â€¦,â€†<em>X</em><sub><em>n</em></sub><sup>(<em>m</em>)</sup></span>,
where $X_i^{(m)} $ represents the number of balls in bin <span
class="math inline"><em>i</em></span>, is the same as <span
class="math inline"><em>Y</em><sub>1</sub><sup>(<em>m</em>)</sup>,â€†â€¦<em>Y</em><sub><em>n</em></sub><sup>(<em>m</em>),</sup>Â <em>i</em><em>i</em><em>d</em></span>,
where <span class="math inline">$Y^{(m)}_i \sim
Pois(\frac{m}{n})$</span> and <span
class="math inline">âˆ‘<em>Y</em><sub><em>i</em></sub><sup>(<em>m</em>)</sup>â€„=â€„<em>k</em></span></p>
<h2 id="proof-5">Proof</h2>
<p>It is based on the fact that <span class="math inline">$X^{(m)}_1
\sim Bin(m, \frac{1}{n}) \approx Pois(\frac{m}{n})$</span> and then we
show that <span
class="math inline"><em>P</em><em>r</em>[<em>X</em><sub>1</sub><sup>(<em>m</em>)</sup>=<em>k</em><sub>1</sub>,â€¦<em>X</em><sub><em>n</em></sub><sup>(<em>m</em>)</sup>=<em>k</em><sub><em>n</em></sub>]â€„=â€„<em>P</em><sub><em>x</em></sub>â€„=â€„<em>P</em><sub><em>Y</em></sub>â€„=â€„<em>P</em><em>r</em>[<em>Y</em><sub>1</sub><sup>(<em>m</em>)</sup>=<em>k</em><sub>1</sub>â€¦|âˆ‘<em>Y</em><sub><em>i</em></sub>=<em>k</em>]</span></p>
<h2 id="max-load-theorem-2">Max Load Theorem 2</h2>
<p>if <span class="math inline"><em>m</em>â€„=â€„<em>n</em></span> and are
big enough and <span class="math inline">$M =
\frac{ln(n)}{ln(ln(n))}$</span> then <span
class="math inline">$Pr[\text{max \# of balls in a bin} &lt; M] \leq
\frac{1}{n}$</span></p>
<h1 id="non-parametric-statistics">Non-parametric statistics</h1>
<p>In parametric statistics, we assume that the data comes from a known
distribution and we try to estimate a parameter of that distribution. In
non-parametric statistics, we donâ€™t assume anything about the
distribution of the data.</p>
<h2 id="permutation-test">Permutation test</h2>
<p>is a technique to decide whether observed random variables come from
the same distribution or not.</p>
<p><span class="math inline">$X_1, \dots, X_m \newline Y_1, \dots,
Y_n$</span></p>
<p><span class="math inline"><em>H</em><sub>0</sub>:</span> All of these
random variables come from the same distribution.</p>
<p>The quantity computed from values in a sample (statistic) <span
class="math inline"><em>T</em></span> is the difference of means of Xs
and Ys.</p>
<p><span
class="math display"><em>T</em>â€„:=â€„<em>XÌ„</em><sub><em>m</em></sub>â€…âˆ’â€…<em>YÌ„</em><sub><em>n</em></sub></span></p>
<p>Alternatively we can use the two-sided test:</p>
<p>$$ T := {X}_m - {Y}_n </p>
<p>$$</p>
<p>We pick a paramater <span class="math inline"><em>Î³</em></span> and
if <span class="math inline"><em>T</em>â€„â‰¥â€„<em>Î³</em></span> then we
reject <span class="math inline"><em>H</em><sub>0</sub></span>. In order
to decide <span class="math inline"><em>Î³</em></span>, we want our test
to be statistically significant. Hence the following must hold: <span
class="math inline"><em>P</em><em>r</em>[wrong
rejection]â€„&lt;â€„<em>Î±</em>â€„=â€„0.05</span></p>
<p>So we will compute <span class="math inline"><em>Î³</em></span> based
on the set of all measured values. Next, the observations of groups
<span class="math inline"><em>X</em></span> and <span
class="math inline"><em>Y</em></span> are pooled, and the difference in
sample means is calculated and recorded for every possible way of
dividing the pooled values into two groups of size <span
class="math inline">|<em>X</em>|</span> and <span
class="math inline">|<em>Y</em>|</span>. The set of these calculated
differences is the exact distribution of possible differences under the
null hypothesis that group labels are exchangeable.The p-value of the
test is calculated as the proportion of sampled permutations where the
difference in means was greater than <span
class="math inline"><em>T</em></span>.</p>
<p>If <span class="math inline">(<em>m</em>+<em>n</em>)!</span> is too
big, we can use a random permutation test. We will generate <span
class="math inline"><em>k</em></span> random permutations and compute
the test statistic for each of them.</p>
<h2 id="one-sampled-sign-test">One-sampled Sign test</h2>
<p>$X_1, X_n Â i.i.d. $ They have unknown distribution which is
continuous, has median <span class="math inline"><em>Î¼</em></span>,
possibly mean <span class="math inline"><em>Î¼</em></span> and is
symmetric around <span class="math inline"><em>Î¼</em></span>.</p>
<p><span
class="math inline"><em>H</em><sub>0</sub>â€„:â€„<em>Î¼</em>â€„=â€„0</span></p>
<p><span
class="math inline"><em>Y</em><sub><em>i</em></sub>â€„=â€„<em>s</em><em>g</em><em>n</em>(<em>X</em><sub><em>i</em></sub>)</span>
is either 1 or 0</p>
<!-- TODO: what happens in case when all Xs are zeros? Do we just skip them? -->
<p><span class="math inline">$Y = \sum Y_i \sim Bin(n,
\frac{1}{2})$</span> assuming <span
class="math inline"><em>H</em><sub>0</sub></span>. Next, we consider the
dsitrubution of <span class="math inline"><em>Y</em></span> and compute
the quantiles based on <span class="math inline"><em>Î±</em></span>. If
<span class="math inline">$Y &gt; y_{1-\frac{\alpha}{2}}$</span> or
<span class="math inline">$Y &lt; y_{\frac{\alpha}{2}}$</span> then we
reject <span class="math inline"><em>H</em><sub>0</sub></span>.</p>
<h2 id="paired-sign-test">Paired Sign test</h2>
<p><span
class="math inline">(<em>X</em><sub>1</sub>,<em>Y</em><sub>2</sub>),â€†â€¦,â€†(<em>X</em><sub><em>n</em></sub>,<em>Y</em><sub><em>n</em></sub>)</span></p>
<p><span
class="math inline"><em>H</em><sub>0</sub>â€„:â€„ğ”¼[<em>X</em>]â€„=â€„ğ”¼[<em>Y</em>]</span>
alternatively <span
class="math inline">ğ”¼[<em>X</em>âˆ’<em>Y</em>]â€„=â€„0</span></p>
<p>We create a new variqble $Z_i = X_i - Y_i $ and we apply the
one-sample sign test on <span
class="math inline"><em>Z</em><sub><em>i</em></sub></span>.</p>
<h2 id="wilcoxon-signed-rank-test">Wilcoxon signed-rank test</h2>
<p>The one-sample Wilcoxon signed-rank test can be used to test whether
data comes from a symmetric population with a specified median.</p>
<p><span
class="math inline"><em>X</em><sub>1</sub>,â€†â€¦,â€†<em>X</em><sub><em>n</em></sub></span>
median is <span class="math inline">0</span></p>
<p><span
class="math inline"><em>H</em><sub>0</sub>â€„:â€„<em>Î¼</em>â€„=â€„0</span></p>
<p>We sort <span
class="math inline">|<em>X</em><sub>1</sub>|,â€†â€¦,â€†|<em>X</em><sub><em>n</em></sub>|</span>
and assign ranks <span
class="math inline"><em>r</em><sub>1</sub>,â€†â€¦,â€†<em>r</em><sub><em>n</em></sub></span>
to them. In case the numbers are the same, we compute the mean of the
range. Then we compute <span class="math inline">$T = \sum_{i=1}^n r_i
sgn(X_i)$</span> which can be computed as <span
class="math inline"><em>T</em>â€„=â€„<em>T</em><sup>+</sup>â€…âˆ’â€…<em>T</em><sup>âˆ’</sup></span></p>
<p>We reject the null hypothesis if <span
class="math inline"><em>T</em></span> is too large or too small.</p>
<h2 id="mann-whitney-u-test">Mann-Whitney U-test</h2>
<p>2-sample non-parametric test, which checks whether two samples come
from the same distribution.</p>
<p>We compute statistics <span class="math inline">$U = \sum^{|X|}_i
\sum^{|Y|}_j S(X_i, Y_j)$</span></p>
<p>where <span
class="math inline"><em>S</em>(<em>X</em><sub><em>i</em></sub>,<em>Y</em><sub><em>j</em></sub>)â€„=â€„0</span>
if <span
class="math inline"><em>X</em><sub><em>i</em></sub>â€„&gt;â€„<em>Y</em><sub><em>j</em></sub></span>,
<span
class="math inline"><em>S</em>(<em>X</em><sub><em>i</em></sub>,<em>Y</em><sub><em>j</em></sub>)â€„=â€„1</span>
when it is the other way around and <span
class="math inline">$\frac{1}{2}$</span> if they are equal.</p>
<p>It is a form of a permutation test.</p>
<h1 id="consequences-of-statistical-designs">Consequences of statistical
designs</h1>
<h2 id="simpsons-paradox">Simpsonâ€™s paradox</h2>
<p>Simpsonâ€™s paradox is a phenomenon in probability and statistics in
which a trend appears in several groups of data but disappears or
reverses when the groups are combined.</p>
<p><strong>Example:</strong> Females at Harvard have overall smaller
success rate than males. However, when compared their success rates in
separate majors, females usually dominate. This means that most of the
females apply to more competitive majors.</p>
<h2 id="time-dependency">Time dependency</h2>
<p><span
class="math inline"><em>X</em><sub>1</sub>,â€†â€¦<em>X</em><sub><em>n</em></sub></span>
all tests assume <span
class="math inline"><em>i</em>.<em>i</em>.<em>d</em>.</span> however, in
reality, the data is dependent. <span
class="math inline">ğ”¼[<em>X</em><sub><em>i</em></sub>]</span> depends on
<span class="math inline"><em>i</em></span>.</p>
<p>We can test this phenomenon by replacing <span
class="math inline"><em>X</em><sub><em>i</em></sub></span> by <span
class="math inline"><em>X</em><sub><em>i</em></sub>â€…âˆ’â€…<em>Î¼</em></span>
where <span class="math inline"><em>Î¼</em></span> is the median of the
measured data. Then we can observe the sequence of pluses and minuses.
If the sequence is random, then we can assume that the data is
independent.</p>
<h1 id="moment-generating-function">Moment Generating Function</h1>
<h2 id="definition-12">Definition</h2>
<p>If <span class="math inline"><em>X</em></span> is a random variable,
<span class="math inline">$s \in \R$</span> then <span
class="math inline"><em>M</em><sub><em>X</em></sub>(<em>s</em>)â€„=â€„ğ”¼[<em>e</em><sup><em>s</em><em>X</em></sup>]</span>
where <span class="math inline"><em>M</em><sub><em>X</em></sub></span>
is the moment generating function of <span
class="math inline"><em>X</em></span>.</p>
<h2 id="theorem-7">Theorem</h2>
<p>For all <span class="math inline"><em>s</em></span> where <span
class="math inline"><em>M</em><sub><em>X</em></sub>(<em>s</em>)</span>
is defined and finite:</p>
<p>$$</p>
<p>M_X(s) = _{k=0}^ [X^k] s^k</p>
<p>$$</p>
<h2 id="proof-6">Proof</h2>
<p><span class="math inline">ğ”¼[<em>X</em><sup><em>k</em></sup>]</span>
is called the <span class="math inline"><em>k</em></span>-th moment.</p>
<p><span
class="math inline">ğ”¼[<em>X</em><sup>2</sup>]â€„=â€„<em>v</em><em>a</em><em>r</em>(<em>X</em>)â€…+â€…ğ”¼[<em>X</em>]<sup>2</sup></span></p>
<p>$$</p>
<p>e^s = _{k=0}^</p>
<p>$$</p>
<p>$$ [e^{sX}] = [^{}_{k=0} ] = ^{}_{k=0} [X^k] s^k</p>
<p>$$</p>
<p>For continuous distribution <span
class="math inline"><em>Y</em></span>, we compute MGF with the help of
LOTUS rule as follows:</p>
<p>$$ M_Y(s) = _{-}<sup>e</sup>{sy} f_Y(y) dy</p>
<p>$$</p>
<h2 id="theorem-8">Theorem</h2>
<p>$$</p>
<p>M_{aX+b}(s) = e^{bs}M_X(as)</p>
<p>$$</p>
<h2 id="theorem-9">Theorem</h2>
<p>if <span class="math inline"><em>X</em></span> and <span
class="math inline"><em>Y</em></span> are independent, then <span
class="math inline"><em>M</em><sub><em>X</em>â€…+â€…<em>Y</em></sub>(<em>s</em>)â€„=â€„<em>M</em><sub><em>X</em></sub>(<em>s</em>)<em>M</em><sub><em>Y</em></sub>(<em>s</em>)</span></p>
<h2 id="theorem-10">Theorem</h2>
<p><span class="math inline">$\exists \epsilon &gt; 0 \forall s \in
[-\epsilon, \epsilon]: M_X(s) = M_Y(s) \in \R \implies F_X =
F_Y$</span></p>
<h2 id="theorem-11">Theorem</h2>
<p><span class="math inline">$\exists \epsilon &gt; 0 \forall s \in
[-\epsilon, \epsilon]: M_{Y_n}(s) \rightarrow M_Z(s) \in \R \ \&amp; \
F_Z \text{ is continuous } \implies F_{Y_n} \rightarrow F_Z$</span></p>
<p>In this case instead of two random variables, we have a sequence of
random variables.</p>
<h2 id="central-limit-theorem">Central Limit Theorem</h2>
<p><span
class="math inline"><em>X</em><sub>1</sub>,â€†â€¦,â€†<em>X</em><sub><em>n</em></sub>Â Â <em>i</em>.<em>i</em>.<em>d</em>.,Â Â ğ”¼[<em>X</em><sub><em>i</em></sub>]â€„=â€„<em>Î¼</em>,Â Â <em>v</em><em>a</em><em>r</em>(<em>X</em><sub><em>i</em></sub>)â€„=â€„<em>Ïƒ</em><sup>2</sup></span></p>
<p><span class="math inline">$Y_n = \frac{1}{\sigma \sqrt{n}}
((\sum_{i=1}^n X_i) - n \mu)$</span></p>
<p>Then <span class="math inline"><em>Y</em><sub><em>n</em></sub></span>
converges to <span class="math inline">ğ’©(0,1)</span></p>
<h2 id="chernooffs-theorem">Chernooffâ€™s theorem</h2>
<p><span class="math inline">$X_1, \dots, X_n \ \ i.i.d., \sim
Bern(\frac{1}{2})$</span></p>
<p><span
class="math inline"><em>X</em>â€„=â€„<em>X</em><sub>1</sub>â€…+â€…â€¦â€…+â€…<em>X</em><sub><em>n</em></sub>,Â Â Â <em>v</em><em>a</em><em>r</em>(<em>X</em>)â€„=â€„<em>n</em></span></p>
<p><span class="math inline">$t &gt; 0: Pr[X \leq -t] = Pr[X \geq t]
\leq e^{\frac{-t^2}{2\sigma^2}}$</span></p>
<h1 id="source-coding-theorem">Source coding theorem</h1>
<p>How to encode the information in the most efficient way?</p>
<p>Model: sequence of <span
class="math inline"><em>X</em><sub>1</sub>,â€†â€¦,â€†<em>X</em><sub><em>n</em></sub><em>i</em>.<em>i</em>.<em>d</em>.</span>
over finite alphabet.</p>
<p>Goal: find the most efficient encoding of the sequence.</p>
<p><span
class="math inline"><em>X</em>â€„=â€„(<em>X</em><sub>1</sub>,â€¦,<em>X</em><sub><em>n</em></sub>)</span></p>
<p><span class="math inline">$L(n \epsilon) = min \{ L : \exists C_n
\sub A^n \ \  s.t. \ |C_n| &lt; 2^l \ \&amp; \ Pr[X \in C_n] \geq 1
-\epsilon \}$</span></p>
<h2 id="shannons-source-coding-theorem">Shannonâ€™s source coding
theorem</h2>
<p><span class="math display">$$
\forall \epsilon &gt; 0: lim_{n \rightarrow \infty} \frac{L(n
\epsilon)}{n} = H(X)
$$</span></p>
